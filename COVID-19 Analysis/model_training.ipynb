{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Infectious Disease Trends & Clustering Healthcare Utilization\n",
    "\n",
    "This notebook will guide you step-by-step through forecasting COVID-19 trends and clustering healthcare utilization patterns across countries.\n",
    "\n",
    "**Goal:**\n",
    "- Predict future infection and healthcare demand trends.\n",
    "- Group countries by similar healthcare usage patterns.\n",
    "\n",
    "*All code and explanations are designed for beginners!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Let's import the libraries we'll need for data analysis, modeling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data handling\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import seaborn as sns  # For prettier plots\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Hide warnings for cleaner output\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data\n",
    "\n",
    "Let's load our cleaned datasets and take a look at what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned datasets\n",
    "train_df = pd.read_csv(\"../data/cleaned_train_data.csv\")\n",
    "test_df = pd.read_csv(\"../data/cleaned_test_data.csv\")\n",
    "\n",
    "# Convert date to datetime\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(f\"Training Data Shape: {train_df.shape}\")\n",
    "print(f\"Test Data Shape: {test_df.shape}\")\n",
    "print(f\"Train date range: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"Test date range: {test_df['date'].min()} to {test_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the training data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data Types:\")\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Before we can build our models, we need to prepare the data. This includes:\n",
    "1. Handling missing values\n",
    "2. Feature engineering\n",
    "3. Encoding categorical variables\n",
    "4. Scaling numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = train_df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "print(\"Missing values in dataset:\")\n",
    "print(missing_values.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns, we'll use median imputation\n",
    "numerical_cols = train_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numerical_cols:\n",
    "    if col in missing_values.index:\n",
    "        train_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "        test_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "\n",
    "# For categorical columns, we'll use mode imputation\n",
    "categorical_cols = train_df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if col in missing_values.index:\n",
    "        train_df[col].fillna(train_df[col].mode()[0], inplace=True)\n",
    "        test_df[col].fillna(train_df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(train_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "# Extract temporal features from date\n",
    "for df in [train_df, test_df]:\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    # Create lagged features for time series\n",
    "    df['new_cases_lag1'] = df.groupby('location')['new_cases'].shift(1)\n",
    "    df['new_cases_lag7'] = df.groupby('location')['new_cases'].shift(7)\n",
    "    df['new_cases_lag14'] = df.groupby('location')['new_cases'].shift(14)\n",
    "    \n",
    "    # Create rolling statistics\n",
    "    df['new_cases_rolling_mean7'] = df.groupby('location')['new_cases'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "    df['new_cases_rolling_std7'] = df.groupby('location')['new_cases'].transform(lambda x: x.rolling(window=7, min_periods=1).std())\n",
    "    \n",
    "    # Create healthcare utilization metric\n",
    "    if 'hospital_beds_per_thousand' in df.columns and 'total_cases' in df.columns:\n",
    "        df['cases_per_bed'] = df['total_cases'] / (df['hospital_beds_per_thousand'] * df['population'] / 1000)\n",
    "        df['healthcare_utilization'] = df['cases_per_bed'].fillna(0)\n",
    "\n",
    "# Display the new features\n",
    "new_features = ['year', 'month', 'day', 'dayofweek', 'quarter', \n",
    "                'new_cases_lag1', 'new_cases_lag7', 'new_cases_lag14',\n",
    "                'new_cases_rolling_mean7', 'new_cases_rolling_std7',\n",
    "                'cases_per_bed', 'healthcare_utilization']\n",
    "train_df[new_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "categorical_cols = ['continent', 'location', 'tests_units']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in train_df.columns:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        train_df[col + '_encoded'] = label_encoders[col].fit_transform(train_df[col].astype(str))\n",
    "        test_df[col + '_encoded'] = label_encoders[col].transform(test_df[col].astype(str))\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_cols = train_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['Id'] and not col.endswith('_encoded')]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n",
    "test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n",
    "\n",
    "print(\"Data preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Forecasting\n",
    "\n",
    "Now that our data is prepared, let's build models to forecast future COVID-19 trends. We'll focus on predicting:\n",
    "1. New cases per day\n",
    "2. New deaths per day\n",
    "3. Healthcare utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variables\n",
    "target_variables = ['new_cases', 'new_deaths', 'healthcare_utilization']\n",
    "\n",
    "# Define features to use for prediction\n",
    "feature_cols = [col for col in train_df.columns if col not in ['Id', 'date', 'iso_code', 'location', 'continent', 'tests_units'] \n",
    "                and not col.endswith('_encoded') and col not in target_variables]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for time series forecasting\n",
    "# We'll use a time-based split to maintain temporal integrity\n",
    "train_cutoff_date = train_df['date'].quantile(0.8)\n",
    "train_forecast = train_df[train_df['date'] < train_cutoff_date].copy()\n",
    "val_forecast = train_df[train_df['date'] >= train_cutoff_date].copy()\n",
    "\n",
    "print(f\"Training data for forecasting: {train_forecast.shape[0]} rows\")\n",
    "print(f\"Validation data for forecasting: {val_forecast.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models for forecasting\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42),\n",
    "    'CatBoost': CatBoostRegressor(iterations=100, random_state=42, verbose=0)\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R\u00b2: {r2:.4f}\")\n",
    "    \n",
    "    return model, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models for each target variable\n",
    "best_models = {}\n",
    "best_scores = {}\n",
    "\n",
    "for target in target_variables:\n",
    "    print(f\"\\nForecasting {target}:\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train_forecast[feature_cols].fillna(0)\n",
    "    y_train = train_forecast[target].fillna(0)\n",
    "    X_val = val_forecast[feature_cols].fillna(0)\n",
    "    y_val = val_forecast[target].fillna(0)\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        trained_model, rmse = evaluate_model(model, X_train, y_train, X_val, y_val, name)\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = trained_model\n",
    "    \n",
    "    best_models[target] = best_model\n",
    "    best_scores[target] = best_rmse\n",
    "    \n",
    "    print(f\"Best model for {target}: {best_model.__class__.__name__} with RMSE: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for the best model for new_cases\n",
    "if 'new_cases' in best_models:\n",
    "    # Get predictions\n",
    "    X_val = val_forecast[feature_cols].fillna(0)\n",
    "    y_val = val_forecast['new_cases'].fillna(0)\n",
    "    y_pred = best_models['new_cases'].predict(X_val)\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(val_forecast['date'], y_val, label='Actual')\n",
    "    plt.plot(val_forecast['date'], y_pred, label='Predicted')\n",
    "    plt.title('New Cases: Actual vs Predicted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('New Cases')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Healthcare Utilization\n",
    "\n",
    "Now, let's cluster countries based on their healthcare utilization patterns. This will help us identify groups of countries with similar healthcare system responses to COVID-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "# We'll use the latest data for each country\n",
    "latest_data = train_df.sort_values('date').groupby('location').last().reset_index()\n",
    "\n",
    "# Select healthcare-related features\n",
    "healthcare_features = [\n",
    "    'hospital_beds_per_thousand',\n",
    "    'life_expectancy',\n",
    "    'human_development_index',\n",
    "    'healthcare_utilization',\n",
    "    'total_cases_per_million',\n",
    "    'total_deaths_per_million'\n",
    "]\n",
    "\n",
    "# Drop rows with missing values\n",
    "healthcare_data = latest_data[['location'] + healthcare_features].dropna()\n",
    "\n",
    "print(f\"Number of countries for clustering: {len(healthcare_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(healthcare_data[healthcare_features])\n",
    "\n",
    "# Apply PCA for dimensionality reduction and visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame with PCA results\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['location'] = healthcare_data['location'].values\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'])\n",
    "plt.title('PCA of Healthcare Features')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Add country labels to some points\n",
    "for i, country in enumerate(pca_df['location']):\n",
    "    if i % 5 == 0:  # Label every 5th country to avoid overcrowding\n",
    "        plt.annotate(country, (pca_df['PC1'][i], pca_df['PC2'][i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters using silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "    silhouette_avg = silhouette_score(scaled_data, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {k}, the average silhouette score is: {silhouette_avg:.4f}\")\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means clustering with the optimal number of clusters\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "healthcare_data['Cluster'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# Visualize clusters in PCA space\n",
    "pca_df['Cluster'] = healthcare_data['Cluster'].values\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Cluster'], cmap='viridis')\n",
    "plt.title('Country Clusters Based on Healthcare Metrics')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Add country labels to some points\n",
    "for i, country in enumerate(pca_df['location']):\n",
    "    if i % 5 == 0:  # Label every 5th country to avoid overcrowding\n",
    "        plt.annotate(country, (pca_df['PC1'][i], pca_df['PC2'][i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "cluster_analysis = healthcare_data.groupby('Cluster')[healthcare_features].mean()\n",
    "print(\"Cluster Characteristics:\")\n",
    "cluster_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countries in each cluster\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    countries = healthcare_data[healthcare_data['Cluster'] == cluster]['location'].tolist()\n",
    "    print(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Insights\n",
    "\n",
    "Let's summarize our findings and provide actionable insights based on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of best forecasting models\n",
    "print(\"Best Forecasting Models:\")\n",
    "for target, model in best_models.items():\n",
    "    print(f\"- {target}: {model.__class__.__name__} with RMSE: {best_scores[target]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of clustering results\n",
    "print(\"\\nClustering Results:\")\n",
    "print(f\"- Optimal number of clusters: {optimal_k}\")\n",
    "print(\"\\nCluster Characteristics:\")\n",
    "for cluster in range(optimal_k):\n",
    "    countries = healthcare_data[healthcare_data['Cluster'] == cluster]['location'].tolist()\n",
    "    print(f\"\\nCluster {cluster} ({len(countries)} countries):\")\n",
    "    print(f\"Countries: {', '.join(countries[:5])}{'...' if len(countries) > 5 else ''}\")\n",
    "    \n",
    "    # Print key characteristics\n",
    "    cluster_data = healthcare_data[healthcare_data['Cluster'] == cluster]\n",
    "    print(f\"Average healthcare utilization: {cluster_data['healthcare_utilization'].mean():.4f}\")\n",
    "    print(f\"Average hospital beds per thousand: {cluster_data['hospital_beds_per_thousand'].mean():.4f}\")\n",
    "    print(f\"Average human development index: {cluster_data['human_development_index'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights:\n",
    "\n",
    "1. **Forecasting Insights**:\n",
    "   - [Your insights will appear here after running the notebook]\n",
    "\n",
    "2. **Clustering Insights**:\n",
    "   - [Your insights will appear here after running the notebook]\n",
    "\n",
    "3. **Recommendations for Healthcare Systems**:\n",
    "   - [Your recommendations will appear here after running the notebook]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}